---
title: "Assignment 2"
output: html_notebook
---

1. (1pt) Download the dataset and store it in a dataframe in R. The dataset does not have header, you should
add the headers manually to your dataframe based on the list of attributes provided in
https://archive.ics.uci.edu/ml/datasets/Adult. Also please note that some entries have extra white space. So
to read the data properly, use the option strip.white=TRUE in read.csv function.

```{r}
adult_df = read.csv("adult.data",header = FALSE,strip.white = TRUE, stringsAsFactors = TRUE,col.names = c("age","workclass","fnlwgt","education","education-num","marital-status","occupation",
                                                                                "relationship","race","sex","capital-gain","capital-loss","hours-per-week","native-country","income"))
```
2. (1pt) Explore the overall structure of the dataset using the str() function. Get a summary statistics of each
variable. How many categorical and numeric variables you have in your data? Is there any missing values?

```{r}
str(adult_df)
```

There are nine categorical variables and six numeric variables in the data set. 

```{r}
summary(adult_df$age)
summary(adult_df$workclass)
summary(adult_df$fnlwgt)
summary(adult_df$education)
summary(adult_df$education.num)
summary(adult_df$marital.status)
summary(adult_df$occupation)
summary(adult_df$relationship)
summary(adult_df$race)
summary(adult_df$sex)
summary(adult_df$capital.gain)
summary(adult_df$capital.loss)
summary(adult_df$hours.per.week)
summary(adult_df$native.country)
summary(adult_df$income)
```

There are 1835 "?" values in the occupation column therefore the data set is missing values

3. (1pt) Get the frequency table of the “income” variable to see how many observations you have in each category of the income variable. Is the data balanced? Do we have equal number of samples in each class of income?
```{r}
inc_table = table(adult_df$income)
inc_table
round(prop.table(inc_table) * 100, digits = 2)
```
The data is skewed heavily to the <=50k with that class making up 76% of all entries

4. (3 pts) Explore the data in order to investigate the association between income and the other features. Which of the other features seem most likely to be useful in predicting income.

4a. To explore the relationship between numerical features and “income” variable, you can use side by side box plot and t.test
```{r}
attach(adult_df)
```
```{r}
boxplot(age~income)
boxplot(fnlwgt~income)
boxplot(education.num~income)
boxplot(education.num~income)
boxplot(capital.gain~income)
boxplot(capital.loss~income)
boxplot(hours.per.week~income)
```
```{r}
t.test(age~income)
t.test(fnlwgt~income)
t.test(education.num~income)
t.test(capital.gain~income)
t.test(capital.loss~income)
t.test(hours.per.week~income)
```

For the numerical features it would appear that there is a relationship between income and the following: age, education-num, capitol-gain, capitol-loss, and hours-per-week
```{r}
adult_df_old = adult_df
adult_df = adult_df[-3]
```


4b. To explore the relationship between categorical features and “income” variable, you can use frequency table and chisquare test (note that chisquare test might throw a warning if there are cells whose expected counts in the frequency table is less 5. This warning means the p-values reported from chisquare test may be incorrect due to low counts and are not reliable. You can ignore the warning for this assignment).

```{r}
workclass_tbl = table(workclass, income)
workclass_tbl
chisq.test(workclass_tbl)

marital.status_tbl = table(marital.status, income)
marital.status_tbl
chisq.test(marital.status_tbl)

education_tbl = table(education, income)
education_tbl
chisq.test(education_tbl)

occupation_tbl = table(occupation, income)
occupation_tbl
chisq.test(occupation_tbl)

relationship_tbl = table(relationship, income)
relationship_tbl
chisq.test(relationship_tbl)

sex_tbl = table(sex, income)
sex_tbl
chisq.test(sex_tbl)

native.country_tbl = table(native.country, income)
native.country_tbl
chisq.test(native.country_tbl[-1]) #to remove "?" from the factor since it breaks the chi-squared test
```

4c.  Based on your data exploration above, decide which attributes you are going to use to predict income.
Explain your reason for selecting these attributes.

I'm using the following attributes because I have shown that there is a statistically significant relationship between them and income: relationship, workclass, marital-status, education, occupation, sex, native-country, age, education-num, capitol-gain, capitol-loss, and hours-per-week

5. (1 pt) An initial data exploration shows that the missing values in the dataset are denoted by “?” not
NA. Change all the “?” characters in the dataframe to NA

```{r}
adult_df[adult_df == "?"] <- NA
```

6. (1pt) Use the command colSums(is.na(<your dataframe>) to get the number of missing values in each
column of your dataframe. Which columns have missing values?

```{r}
colSums(is.na(adult_df))
```
workclass, occupation and native.country have missing values

7. (3 pt) There are several ways we can deal with missing values. The easiest approach is to remove all the
rows with missing values. However, if a large number of rows have missing values removing them will
result in loss of information and may affect the classifier performance. If a large number of rows have
missing values, then it is typically better to replace missing data with some values. This is called data
imputation. Several methods for missing data imputation exist. The most naïve method (which we will use
here) is to replace the missing values with mean of the column (for a numerical column) or mode/majority
value of the column (for a categorical column). We will use a more advanced data imputation method in a
later module. For now, replace the missing values in a numerical column with the mean of the column and
the missing values in a categorical column with the mode/majority of the column. After imputation, use
colSums(is.na(<your dataframe>) to make sure that your dataframe no longer has missing values

```{r}
Mode <- function(x) {
  ux <- unique(x)
  ux[which.max(tabulate(match(x, ux)))]
}
```
```{r}
adult_df$workclass[is.na(adult_df$workclass)] = Mode(adult_df$workclass)
adult_df$occupation[is.na(adult_df$occupation)] = Mode(adult_df$occupation)
adult_df$native.country[is.na(adult_df$native.country)] = Mode(adult_df$native.country)
```
```{r}
colSums(is.na(adult_df))
```

8. (2 pt) This dataset has several categorical variables. With the exception of few models ( such as Naiive Bayes and tree-based models) most machine learning models require numeric features and cannot work directly with categorical data. One way to deal with categorical variables is to assign numeric indices to each level. However, this imposes an artificial ordering on an unordered categorical variable. For example, suppose that we have a categorical variable primary color with three levels: “red”,”blue”,”green”. If we convert “red” to 0 , “blue” to 1 and “green” to 2 then we are telling our model that red < blue< green which is not correct. A better way to encode an unordered categorical variable is to do one-hot-encoding. In one hot-encoding we create a dummy binary variable for each level of a categorical variable. For example we can represent the primary color variable by three binary dummy variables, one for each color (red, blue, and green) . If the color is red, then the variable red takes value 1 while blue and green both take the value zero.

Do one-hot-encoding of all your unordered categorical variables (except the income variable). You can use the function one_hot from mltools package to one-hot encode all categorical variables in a dataset. Please refer to https://rdrr.io/cran/mltools/man/one_hot.html . Use option DropUnusedLevels=True to avoid creating a binary variable for unused levels of a factor variable.

Please note that the one_hot function takes a data table not a dataframe. You can convert a dataframe to datatable by using as.data.table method https://www.rdocumentation.org/packages/data.table/versions/1.12.8/topics/as.data.table. Make sure to use library(data.table) before using as.data.table method. You can covert a datatable back to a dataframe by using as.data.frame method https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/as.data.frame

```{r}
#install.packages("mltools")
library(mltools)
library(data.table)
```

```{r}
encoded_table = one_hot(data.table(adult_df[,-which(names(adult_df) == "income")]), dropUnusedLevels = TRUE)
encoded_df = as.data.frame(encoded_table)
encoded_df$income = adult_df$income
names(encoded_df)
```

9. Set the seed of the random number generator to a fixed integer, say 1, so that I can reproduce your work:
```{r}
set.seed(1)
```


10. (1 pt) Scale all numeric features using Min-Max scaling

```{r}
normalize = function(col){
    (col - min(col))/(max(col) - min(col))
}
normalized_df = as.data.frame(sapply(encoded_df[-(length(encoded_df))], normalize))
```

11. (1pt) Randomize the order of the rows in the dataset.

```{r}
encoded_df = encoded_df[sample(nrow(encoded_df),replace = FALSE),]
normalized_df = as.data.frame(sapply(encoded_df[-(length(encoded_df))], normalize))
```

12. (4 pts) Use 5-fold cross validation with KNN to predict the “income” variable and report the cross-
validation error. ( You can find an example in slides 51-53 of module 4 lecture notes).

```{r}
library(lattice)
library(ggplot2)
library(caret)
library(class)
```

```{r}
knn_fold=function(features,target,fold,k){
  train=features[-fold,]
  validation=features[fold,]
  train_labels=target[-fold]
  validation_labels=target[fold]
  validation_preds=knn(train,validation,train_labels,k=k)
  t= table(validation_labels,validation_preds)
  error=(t[1,2]+t[2,1])/(t[1,1]+t[1,2]+t[2,1]+t[2,2])
  return(error)
}

crossValidationError=function(features,target,k,k_cross){
  folds=createFolds(target,k=k_cross)
  errors=sapply(folds,knn_fold,features=features, target=target,k=k)
}
```

```{r}
cf_error_180 = crossValidationError(normalized_df,encoded_df$income, k = 180, k_cross=5) #sqrt of the nrow of normalized_df = 180
```

```{r}
mean(cf_error_180)
```

13. (2 pts) Tune K (the number of nearest neighbors) by trying out different values (starting from k=1 to
k=sqrt(n) where n is the number of observations in the dataset (for example k=1,5,10,20 50,100, sqrt(n) ).
Draw a plot of cross validation errors for different values of K. Which value of K seems to perform the best
on this data set? (You can find an example in slides 54-55 of module 4 lecture notes) Note: This might a
long time to run on your machine, be patient ( It took about 30 minutes on my machine to run 5-fold cross
validation for 6 different K values)

```{r}
#k = 90
cf_error_90 = crossValidationError(normalized_df,encoded_df$income, k = 90, k_cross=5)
mean(cf_error_90)
```
```{r}
#k = 45
cf_error_45 = crossValidationError(normalized_df,encoded_df$income, k = 45, k_cross=5)
mean(cf_error_45)
```

```{r}
#k = 22
cf_error_22 = crossValidationError(normalized_df,encoded_df$income, k = 22, k_cross=5)
mean(cf_error_22)
```

```{r}
#k = 11
cf_error_11 = crossValidationError(normalized_df,encoded_df$income, k = 11, k_cross=5)
mean(cf_error_11)
```

```{r}
#k = 5
cf_error_5 = crossValidationError(normalized_df,encoded_df$income, k = 5, k_cross=5)
mean(cf_error_5)
```

```{r}
#k = 2
cf_error_2 = crossValidationError(normalized_df,encoded_df$income, k = 2, k_cross=5)
mean(cf_error_2)
```


```{r}
#k = 1
cf_error_1 = crossValidationError(normalized_df,encoded_df$income, k = 1, k_cross=5)
mean(cf_error_1)
```
```{r}
error = c(mean(cf_error_1), mean(cf_error_2), mean(cf_error_5), mean(cf_error_11), mean(cf_error_22), mean(cf_error_45), mean(cf_error_90), mean(cf_error_180))
kvals = c(1, 2, 5, 11, 22, 45, 90, 180)
plot(kvals, error)
```

14. (3 pt) Use 5-fold cross validation with KNN to predict the income variable and report the average false
positive rate (FPR) and false negative rate (FNR) of the classifier. . FPR is the proportion of negative
instances classified as positive by the classifier. Similarly, FNR is the proportion of positive instances
classified as negative by the classifier.

It does not matter which class you designate as positive or negative. For instance, you can designate
income>50K as positive and income<50K as negative
Note : don’t need to tune K again, you can use the best K you found in the previous question.
Note: you can find an example of computing false negative rate and false positive rate for each fold in module
5 lecture notes slides 56-59. These slides use a naiive bayes classifier for prediction but you can modify it to
use knn and report the average FPR and FNR

```{r}
knn_fold = function(features,target,fold,k){
  train=features[-fold,]
  validation=features[fold,]
  train_labels=target[-fold]
  validation_labels=target[fold]
  validation_preds=knn(train,validation,train_labels,k=k)
  t= table(validation_labels,validation_preds)
  FPR=t[1,2]/(t[1,2]+t[1,1])
  FNR=t[2,1]/(t[2,1]+t[2,2])
  return (c("FPR"=FPR,"FNR"=FNR))
}

crossValidationError=function(features,target,k,k_cross){
  folds=createFolds(target,k=k_cross)
  errors=sapply(folds,knn_kfold,features=features, target=target,k=k)
}

errors = crossValidationError(normalized_df,encoded_df$income, k = 22, k_cross=5)

rowMeans(errors)
```

15. (2 pt) Consider a majority classifier which always predicts income <50K. Without writing any code, explain
what would be the training error of this classifier? ( Note the training error of this majority classifier is simply the
proportion of all examples with income>50K because they are all misclassified by this majority classifier).
Compare this with the cross validation error of KNN you computed in question 8. Does KNN do better than this
majority classifier?

The training error would be 7841/32561 = 0.2408 = 24.08%
the lowest training error with knn is 0.166119 = 16.6% which means knn does better than majority classifier 

16. (2 pt) Explain what is the False Positive Rate and False Negative Rate of the majority classifier and how
does it compare to the average FPR and FNR of KNN classifier you computed in question 10. You don’t
need to write any code to compute FPR and FNR of the majority classifier. You can just compute it based
on the definition of FNR and FPR.

if income <50k is positive then all negatives (>50k) are misidentified as positives which gives a FPR is 100%
and therefore no positives are misidentified as false negatives giving a FNR of 0%

Problem 2: Applying Naïve Bayes classifier to predict income

1. Set the seed of the random number generator to a fixed integer, say 1, so that you can
reproduce your work:

```{r}
set.seed(1)
```

2. Based on your data exploration in problem1, decide which features you want to keep to predict the
income and remove other features.

I'm sticking with the columns used with knn


Note: As we learned in the lectures, Naïve Bayes uses frequency tables to compute conditional
probabilities. Frequency tables are only computed for categorical (or discrete features). For a
continuous feature, you can either
1. manually convert it to a categorical variable by binning ( as explained in the lectures), or
2. pass it directly to naiveBayes() function in R and it will use Gaussian Naïve Bayes to
compute the conditional probabilities. More specifically, If xx is a continuous feature and yy is
the target variable ( i.e., the variable we want to predict) with cc 1 , cc 2 , ... , cc kk classes, then the
Gaussian Naïve Bayes assumes that the likelihood pp(xx|yy = CC ii ) has a Gaussian/normal
distribution and estimates its mean and variance from the training data. This distribution is
used (instead of a frequency table) to compute the likelihood for each observed value of xx in
the data. Fortunately, the naiveBayes function in R does this computation for you. You just
pass the dataset as is and it will automatically use Gaussian Naïve Bayes to compute
conditional probabilities for continuous features.

3. (4 pt) Use 5-fold cross validation with Naïve Bayes to predict the “income” variable and report the
cross-validation error.

```{r}
#install.packages("e1071")
library(e1071)
```

```{r}
naiveBayes_fold=function(fold,features,target,laplace=0){
  train=features[-fold,]
  validation=features[fold,]
  train_labels=target[-fold]
  validation_labels=target[fold]
  NaiveBayes_model=naiveBayes(train,train_labels,laplace=laplace)
  validation_preds=predict(NaiveBayes_model, validation)
  t= table(validation_labels,validation_preds)
  error=(t[1,2]+t[2,1])/(t[1,1]+t[1,2]+t[2,1]+t[2,2])
  return(error)
}

crossValidationError=function(features,target,laplace=0,n_folds)
{
  folds=createFolds(target,k=n_folds)
  errors=sapply(folds,naiveBayes_fold,features=features,
  target=target,laplace=laplace)
}

cv_error = crossValidationError(adult_df, adult_df$income, n_folds = 5)
mean(cv_error)
```
2.6% error rate

4. (2 pt)Compare the cross validation error of Naiive Bayes with that of KNN. Which one performs better
on this dataset?

at best I found that knn performs with a 16% error rate, which is significantly worse than the 2.6% error rate for naive bayes


5. (2 pt) Compare the False Positive Rate and False Negative Rate of naiive Bayes with those of the
majority classifier which always predicts income <50K.

```{r}
naiveBayes_fold=function(fold,features,target,laplace=0){
  train=features[-fold,]
  validation=features[fold,]
  train_labels=target[-fold]
  validation_labels=target[fold]
  NaiveBayes_model=naiveBayes(train,train_labels,laplace=laplace)
  validation_preds=predict(NaiveBayes_model, validation)
  t= table(validation_labels,validation_preds)
  FPR=t[1,2]/(t[1,2]+t[1,1])
  FNR=t[2,1]/(t[2,1]+t[2,2])
  return (c("FPR"=FPR,"FNR"=FNR))
}

crossValidationError=function(features,target,laplace=0,n_folds)
{
  folds=createFolds(target,k=n_folds)
  errors=sapply(folds,naiveBayes_fold,features=features,
  target=target,laplace=laplace)
  return(rowMeans(errors))
}

errors = crossValidationError(adult_df, adult_df$income, n_folds = 5)
errors
```
a FPR is 100% for the majority classifier is the worst rate possible so naive bayes wins out in that category. However a respectable 5.2% FNR for naive bayes is not quite as good as the 0% FNR for majority classifier





